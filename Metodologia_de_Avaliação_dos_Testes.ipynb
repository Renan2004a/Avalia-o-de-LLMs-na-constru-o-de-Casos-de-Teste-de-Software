{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renan2004a/Avalia-o-de-LLMs-na-constru-o-de-Casos-de-Teste-de-Software/blob/main/Metodologia_de_Avalia%C3%A7%C3%A3o_dos_Testes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# ========== CONFIGURAÃ‡ÃƒO ==========\n",
        "arquivos_llms = {\n",
        "    'Claude_Sonnet': 'Claude_Sonnet_4_5.py',\n",
        "    'DeepSeek_V3': 'DeepSeek-V3.py',\n",
        "    'Copilot_Smart': 'Copilot_Smart _(GPTâ€‘5).py',\n",
        "    'Gemini_1_5': 'Gemini_1_5_Flash.py',\n",
        "    'GPT_4o_mini': 'GPT-4o_mini.py'\n",
        "}\n",
        "\n",
        "# ========== FUNÃ‡Ã•ES DE ANÃLISE ==========\n",
        "\n",
        "def analisar_arquivo_testes(caminho_arquivo):\n",
        "    \"\"\"Analisa um arquivo de testes e extrai mÃ©tricas\"\"\"\n",
        "    try:\n",
        "        with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
        "            conteudo = f.read()\n",
        "\n",
        "        metrics = {\n",
        "            'total_testes': 0,\n",
        "            'testes_sucesso': 0,\n",
        "            'testes_erro': 0,\n",
        "            'testes_borda': 0,\n",
        "            'testes_metodo': 0,\n",
        "            'cobertura_cenarios': set(),\n",
        "            'complexidade_testes': 0,\n",
        "            'comentarios_por_teste': 0,\n",
        "            'linhas_por_teste': 0,\n",
        "            'asserts_por_teste': 0,\n",
        "            'erros_cobertos': set(),\n",
        "            'metodos_cobertos': set()\n",
        "        }\n",
        "\n",
        "        # AnÃ¡lise por regex para identificar padrÃµes de teste\n",
        "        padrao_funcao_teste = r'def (test_[^(]+)\\([^)]*\\):'\n",
        "        funcoes_teste = re.findall(padrao_funcao_teste, conteudo)\n",
        "        metrics['total_testes'] = len(funcoes_teste)\n",
        "\n",
        "        # AnÃ¡lise de cenÃ¡rios cobertos\n",
        "        for func in funcoes_teste:\n",
        "            func_lower = func.lower()\n",
        "\n",
        "            # Classificar tipo de teste\n",
        "            if any(palavra in func_lower for palavra in ['sucesso', 'valido', 'happy']):\n",
        "                metrics['testes_sucesso'] += 1\n",
        "            elif any(palavra in func_lower for palavra in ['erro', 'invalido', 'error', 'validation']):\n",
        "                metrics['testes_erro'] += 1\n",
        "                # Extrair tipo de erro\n",
        "                if 'nome' in func_lower:\n",
        "                    metrics['erros_cobertos'].add('nome')\n",
        "                elif 'email' in func_lower:\n",
        "                    metrics['erros_cobertos'].add('email')\n",
        "                elif 'idade' in func_lower:\n",
        "                    metrics['erros_cobertos'].add('idade')\n",
        "                elif 'ativo' in func_lower:\n",
        "                    metrics['erros_cobertos'].add('ativo')\n",
        "                elif 'duplicado' in func_lower:\n",
        "                    metrics['erros_cobertos'].add('id_duplicado')\n",
        "            elif any(palavra in func_lower for palavra in ['borda', 'limite', 'edge']):\n",
        "                metrics['testes_borda'] += 1\n",
        "            elif any(palavra in func_lower for palavra in ['metodo', 'criar', 'buscar', 'atualizar', 'excluir']):\n",
        "                metrics['testes_metodo'] += 1\n",
        "                # Identificar mÃ©todo coberto\n",
        "                if 'criar' in func_lower:\n",
        "                    metrics['metodos_cobertos'].add('criarUsuario')\n",
        "                elif 'buscar' in func_lower:\n",
        "                    metrics['metodos_cobertos'].add('buscarUsuario')\n",
        "                elif 'atualizar' in func_lower:\n",
        "                    metrics['metodos_cobertos'].add('atualizarUsuario')\n",
        "                elif 'excluir' in func_lower:\n",
        "                    metrics['metodos_cobertos'].add('excluirUsuario')\n",
        "\n",
        "            # CenÃ¡rios cobertos\n",
        "            if 'nome' in func_lower:\n",
        "                metrics['cobertura_cenarios'].add('validaÃ§Ã£o_nome')\n",
        "            if 'email' in func_lower:\n",
        "                metrics['cobertura_cenarios'].add('validaÃ§Ã£o_email')\n",
        "            if 'idade' in func_lower:\n",
        "                metrics['cobertura_cenarios'].add('validaÃ§Ã£o_idade')\n",
        "            if 'ativo' in func_lower:\n",
        "                metrics['cobertura_cenarios'].add('validaÃ§Ã£o_ativo')\n",
        "            if 'id' in func_lower and 'duplicado' in func_lower:\n",
        "                metrics['cobertura_cenarios'].add('id_duplicado')\n",
        "\n",
        "        # AnÃ¡lise de complexidade (contagem de asserts)\n",
        "        asserts = re.findall(r'assert ', conteudo)\n",
        "        metrics['asserts_por_teste'] = len(asserts) / metrics['total_testes'] if metrics['total_testes'] > 0 else 0\n",
        "\n",
        "        # AnÃ¡lise de comentÃ¡rios\n",
        "        comentarios = re.findall(r'\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\'|#.*?$', conteudo, re.MULTILINE | re.DOTALL)\n",
        "        metrics['comentarios_por_teste'] = len(comentarios) / metrics['total_testes'] if metrics['total_testes'] > 0 else 0\n",
        "\n",
        "        # Converter sets para contagens (mantemos os sets originais tambÃ©m para anÃ¡lise qualitativa)\n",
        "        metrics['num_erros_cobertos'] = len(metrics['erros_cobertos'])\n",
        "        metrics['num_metodos_cobertos'] = len(metrics['metodos_cobertos'])\n",
        "        metrics['num_cobertura_cenarios'] = len(metrics['cobertura_cenarios'])\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao analisar {caminho_arquivo}: {e}\")\n",
        "        return None\n",
        "\n",
        "# ========== ANÃLISE COMPARATIVA ==========\n",
        "\n",
        "def calcular_pontuacao_final(metrics_dict, resultados_completos):\n",
        "    \"\"\"Calcula uma pontuaÃ§Ã£o final baseada em mÃºltiplas mÃ©tricas\"\"\"\n",
        "    if not metrics_dict:\n",
        "        return 0\n",
        "\n",
        "    pesos = {\n",
        "        'total_testes': 0.15,\n",
        "        'num_cobertura_cenarios': 0.20,\n",
        "        'testes_erro': 0.15,\n",
        "        'testes_borda': 0.15,\n",
        "        'num_metodos_cobertos': 0.10,\n",
        "        'num_erros_cobertos': 0.10,\n",
        "        'asserts_por_teste': 0.10,\n",
        "        'comentarios_por_teste': 0.05\n",
        "    }\n",
        "\n",
        "    # Encontrar valores mÃ¡ximos para normalizaÃ§Ã£o\n",
        "    valores_validos = [m for m in resultados_completos.values() if m is not None]\n",
        "\n",
        "    max_testes = max(m['total_testes'] for m in valores_validos) if valores_validos else 1\n",
        "    max_cenarios = max(m['num_cobertura_cenarios'] for m in valores_validos) if valores_validos else 1\n",
        "    max_erros = max(m['testes_erro'] for m in valores_validos) if valores_validos else 1\n",
        "    max_borda = max(m['testes_borda'] for m in valores_validos) if valores_validos else 1\n",
        "    max_metodos = max(m['num_metodos_cobertos'] for m in valores_validos) if valores_validos else 1\n",
        "    max_erros_cobertos = max(m['num_erros_cobertos'] for m in valores_validos) if valores_validos else 1\n",
        "    max_asserts = max(m['asserts_por_teste'] for m in valores_validos) if valores_validos else 1\n",
        "    max_comentarios = max(m['comentarios_por_teste'] for m in valores_validos) if valores_validos else 1\n",
        "\n",
        "    pontuacao = 0\n",
        "    if max_testes > 0:\n",
        "        pontuacao += pesos['total_testes'] * (metrics_dict['total_testes'] / max_testes)\n",
        "    if max_cenarios > 0:\n",
        "        pontuacao += pesos['num_cobertura_cenarios'] * (metrics_dict['num_cobertura_cenarios'] / max_cenarios)\n",
        "    if max_erros > 0:\n",
        "        pontuacao += pesos['testes_erro'] * (metrics_dict['testes_erro'] / max_erros)\n",
        "    if max_borda > 0:\n",
        "        pontuacao += pesos['testes_borda'] * (metrics_dict['testes_borda'] / max_borda)\n",
        "    if max_metodos > 0:\n",
        "        pontuacao += pesos['num_metodos_cobertos'] * (metrics_dict['num_metodos_cobertos'] / max_metodos)\n",
        "    if max_erros_cobertos > 0:\n",
        "        pontuacao += pesos['num_erros_cobertos'] * (metrics_dict['num_erros_cobertos'] / max_erros_cobertos)\n",
        "    if max_asserts > 0:\n",
        "        pontuacao += pesos['asserts_por_teste'] * (metrics_dict['asserts_por_teste'] / max_asserts)\n",
        "    if max_comentarios > 0:\n",
        "        pontuacao += pesos['comentarios_por_teste'] * (metrics_dict['comentarios_por_teste'] / max_comentarios)\n",
        "\n",
        "    return pontuacao * 100  # Converter para porcentagem\n",
        "\n",
        "# ========== EXECUTAR ANÃLISE ==========\n",
        "\n",
        "print(\"Iniciando anÃ¡lise dos arquivos de teste...\")\n",
        "resultados = {}\n",
        "for llm, arquivo in arquivos_llms.items():\n",
        "    print(f\"Analisando {llm}...\")\n",
        "    resultados[llm] = analisar_arquivo_testes(arquivo)\n",
        "\n",
        "# Criar DataFrame com resultados (usando as mÃ©tricas numÃ©ricas)\n",
        "dados_para_dataframe = {}\n",
        "for llm, metrics in resultados.items():\n",
        "    if metrics:\n",
        "        dados_para_dataframe[llm] = {\n",
        "            'total_testes': metrics['total_testes'],\n",
        "            'testes_sucesso': metrics['testes_sucesso'],\n",
        "            'testes_erro': metrics['testes_erro'],\n",
        "            'testes_borda': metrics['testes_borda'],\n",
        "            'testes_metodo': metrics['testes_metodo'],\n",
        "            'cobertura_cenarios': metrics['num_cobertura_cenarios'],\n",
        "            'asserts_por_teste': metrics['asserts_por_teste'],\n",
        "            'comentarios_por_teste': metrics['comentarios_por_teste'],\n",
        "            'metodos_cobertos': metrics['num_metodos_cobertos'],\n",
        "            'erros_cobertos': metrics['num_erros_cobertos']\n",
        "        }\n",
        "\n",
        "df_metrics = pd.DataFrame.from_dict(dados_para_dataframe, orient='index')\n",
        "df_metrics = df_metrics.fillna(0)\n",
        "\n",
        "# Calcular pontuaÃ§Ãµes finais\n",
        "pontuacoes = {}\n",
        "for llm in df_metrics.index:\n",
        "    metrics_dict = resultados[llm]\n",
        "    if metrics_dict:\n",
        "        pontuacoes[llm] = calcular_pontuacao_final(metrics_dict, resultados)\n",
        "    else:\n",
        "        pontuacoes[llm] = 0\n",
        "\n",
        "df_metrics['pontuacao_final'] = df_metrics.index.map(pontuacoes)\n",
        "\n",
        "# Ordenar por pontuaÃ§Ã£o\n",
        "df_metrics = df_metrics.sort_values('pontuacao_final', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTADO DA COMPARAÃ‡ÃƒO DE LLMs\")\n",
        "print(\"=\"*80)\n",
        "print(df_metrics.round(2))\n",
        "\n",
        "# ========== VISUALIZAÃ‡ÃƒO DETALHADA ==========\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANÃLISE DETALHADA POR LLM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for llm in df_metrics.index:\n",
        "    metrics = resultados[llm]\n",
        "    if metrics:\n",
        "        print(f\"\\n--- {llm} ---\")\n",
        "        print(f\"PontuaÃ§Ã£o Final: {df_metrics.loc[llm, 'pontuacao_final']:.1f}%\")\n",
        "        print(f\"Total de Testes: {metrics['total_testes']}\")\n",
        "        print(f\"Testes de Sucesso: {metrics['testes_sucesso']}\")\n",
        "        print(f\"Testes de Erro: {metrics['testes_erro']}\")\n",
        "        print(f\"Testes de Borda: {metrics['testes_borda']}\")\n",
        "        print(f\"Testes por MÃ©todo: {metrics['testes_metodo']}\")\n",
        "        print(f\"CenÃ¡rios Cobertos: {metrics['num_cobertura_cenarios']}\")\n",
        "        print(f\"MÃ©todos Cobertos: {metrics['num_metodos_cobertos']}/4\")\n",
        "        print(f\"Tipos de Erro Cobertos: {metrics['num_erros_cobertos']}/5\")\n",
        "        print(f\"Asserts por Teste: {metrics['asserts_por_teste']:.1f}\")\n",
        "        print(f\"ComentÃ¡rios por Teste: {metrics['comentarios_por_teste']:.1f}\")\n",
        "\n",
        "# ========== RANKING FINAL ==========\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ† RANKING FINAL DOS LLMs\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "ranking = df_metrics['pontuacao_final'].sort_values(ascending=False)\n",
        "for i, (llm, score) in enumerate(ranking.items(), 1):\n",
        "    print(f\"{i}Âº Lugar: {llm} - {score:.1f}%\")\n",
        "\n",
        "# ========== ANÃLISE QUALITATIVA ==========\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“Š ANÃLISE QUALITATIVA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Verificar cobertura completa dos mÃ©todos\n",
        "metodos_esperados = {'criarUsuario', 'buscarUsuario', 'atualizarUsuario', 'excluirUsuario'}\n",
        "for llm in df_metrics.index:\n",
        "    metrics = resultados[llm]\n",
        "    if metrics:\n",
        "        metodos_cobertos = metrics['metodos_cobertos']  # Este Ã© o set original\n",
        "        cobertura = len(metodos_cobertos) / len(metodos_esperados) * 100\n",
        "        print(f\"{llm}: {cobertura:.0f}% de cobertura de mÃ©todos ({len(metodos_cobertos)}/{len(metodos_esperados)})\")\n",
        "        if metodos_cobertos:\n",
        "            print(f\"  MÃ©todos testados: {', '.join(sorted(metodos_cobertos))}\")\n",
        "\n",
        "# Verificar diversidade de testes\n",
        "print(f\"\\nðŸ“ˆ Diversidade de Testes (Erro/Sucesso/Borda):\")\n",
        "for llm in df_metrics.index:\n",
        "    metrics = resultados[llm]\n",
        "    if metrics:\n",
        "        total = metrics['total_testes']\n",
        "        if total > 0:\n",
        "            pct_erro = (metrics['testes_erro'] / total) * 100\n",
        "            pct_sucesso = (metrics['testes_sucesso'] / total) * 100\n",
        "            pct_borda = (metrics['testes_borda'] / total) * 100\n",
        "            outros = 100 - pct_erro - pct_sucesso - pct_borda\n",
        "            print(f\"{llm}: Erro({pct_erro:.0f}%) Sucesso({pct_sucesso:.0f}%) Borda({pct_borda:.0f}%) Outros({outros:.0f}%)\")\n",
        "\n",
        "# AnÃ¡lise de cenÃ¡rios cobertos\n",
        "print(f\"\\nðŸŽ¯ CenÃ¡rios de ValidaÃ§Ã£o Cobertos:\")\n",
        "cenarios_esperados = {'validaÃ§Ã£o_nome', 'validaÃ§Ã£o_email', 'validaÃ§Ã£o_idade', 'validaÃ§Ã£o_ativo', 'id_duplicado'}\n",
        "for llm in df_metrics.index:\n",
        "    metrics = resultados[llm]\n",
        "    if metrics:\n",
        "        cenarios_cobertos = metrics['cobertura_cenarios']\n",
        "        cobertura = len(cenarios_cobertos) / len(cenarios_esperados) * 100\n",
        "        print(f\"{llm}: {cobertura:.0f}% de cenÃ¡rios ({len(cenarios_cobertos)}/{len(cenarios_esperados)})\")\n",
        "        if cenarios_cobertos:\n",
        "            print(f\"  CenÃ¡rios: {', '.join(sorted(cenarios_cobertos))}\")\n",
        "\n",
        "# Resumo final\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸŽ¯ RESUMO DAS FORÃ‡AS E FRAQUEZAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for llm in df_metrics.index:\n",
        "    metrics = resultados[llm]\n",
        "    if metrics:\n",
        "        print(f\"\\n{llm}:\")\n",
        "        pontos_fortes = []\n",
        "        pontos_fracos = []\n",
        "\n",
        "        if metrics['total_testes'] >= 20:\n",
        "            pontos_fortes.append(\"Alta quantidade de testes\")\n",
        "        elif metrics['total_testes'] < 10:\n",
        "            pontos_fracos.append(\"Poucos testes\")\n",
        "\n",
        "        if metrics['num_metodos_cobertos'] == 4:\n",
        "            pontos_fortes.append(\"Cobertura completa de mÃ©todos\")\n",
        "        else:\n",
        "            pontos_fracos.append(f\"Cobertura parcial de mÃ©todos ({metrics['num_metodos_cobertos']}/4)\")\n",
        "\n",
        "        if metrics['num_erros_cobertos'] >= 4:\n",
        "            pontos_fortes.append(\"Boa cobertura de erros\")\n",
        "        elif metrics['num_erros_cobertos'] < 3:\n",
        "            pontos_fracos.append(\"Cobertura limitada de erros\")\n",
        "\n",
        "        if metrics['asserts_por_teste'] >= 2:\n",
        "            pontos_fortes.append(\"Testes robustos (muitos asserts)\")\n",
        "        elif metrics['asserts_por_teste'] < 1:\n",
        "            pontos_fracos.append(\"Testes pouco assertivos\")\n",
        "\n",
        "        if pontos_fortes:\n",
        "            print(f\"  âœ… ForÃ§as: {', '.join(pontos_fortes)}\")\n",
        "        if pontos_fracos:\n",
        "            print(f\"  âŒ Fraquezas: {', '.join(pontos_fracos)}\")"
      ],
      "metadata": {
        "id": "gKRymbRxtP2y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}